{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Nicol√°s Acevedo\n",
    "- Fabiola Pizarro\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: `https://github.com/nicoacevedor/MDS7202`\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Importaci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as nativas de Python\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings \n",
    "\n",
    "# Librer√≠as Third-Party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_datetime64_any_dtype\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import kaleido\n",
    "import scipy.stats as ss\n",
    "import sklearn.cluster as skl_cl\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.preprocessing as skl_pre\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\"Los datos deben estar en formato Pandas DataFrame\")\n",
    "        self.df_raw = df\n",
    "        self.df_processed = None\n",
    "        self.today = datetime.date.today().strftime('%d-%m-%Y')\n",
    "        self.root_path = Path(f\"EDA_{self.today}\")\n",
    "        self.root_path.mkdir(exist_ok=True)\n",
    "        self.numeric_list, self.datetime_list, self.categorical_list = [], [], []\n",
    "        for col in df.columns:\n",
    "            if is_numeric_dtype(df[col]):\n",
    "                self.numeric_list.append(df.columns.get_loc(col))\n",
    "            elif is_datetime64_any_dtype(df[col]):\n",
    "                self.datetime_list.append(df.columns.get_loc(col))\n",
    "            elif is_categorical_dtype(df[col]):\n",
    "                self.categorical_list.append(df.columns.get_loc(col))\n",
    "\n",
    "\n",
    "    def _get_Z_score(self, value, mean, std):\n",
    "        return (value - mean)/std\n",
    "\n",
    "\n",
    "    def summarize(self, subset=[], anomaly_threshold=None, anomalies=None, show_anomalies=False):\n",
    "        if anomaly_threshold and anomalies:\n",
    "            raise ValueError(\"'anomaly_threshold' se aplica a todas las columnas num√©ricas, mientras que 'anomalies' se aplica a la columna indicada. No pueden ser ingresados ambos.\")\n",
    "        df_in = self.df_raw if not subset else self.df_raw[subset]\n",
    "        types = df_in.dtypes\n",
    "        unique_values = df_in.apply(lambda col: len(col.unique()))\n",
    "        unique_values_per = df_in.apply(lambda col: f\"{(len(col.unique())*100/len(col)):.2f}%\")\n",
    "        null_values = df_in.apply(lambda col: col.isna().sum())\n",
    "        null_values_per = df_in.apply(lambda col: f\"{(col.isna().sum()*100/len(col)):.2f}%\")\n",
    "        \n",
    "        all_text = []\n",
    "        for index, col in enumerate(df_in.columns):\n",
    "            col_text = \"-\"*20 + f\"\\nResumen variable '{col}':\\n\\n\" \\\n",
    "                    + f\"Tipo: {types[col]}\\n\" \\\n",
    "                    + f\"Cantidad valores √∫nicos: {unique_values[col]}\\n\" \\\n",
    "                    + f\"Porcentaje valores √∫nicos: {unique_values_per[col]}\\n\" \\\n",
    "                    + f\"Cantidad valores nulos: {null_values[col]}\\n\" \\\n",
    "                    + f\"Porcentaje valores nulos: {null_values_per[col]}\\n\\n\"\n",
    "            \n",
    "            if index in self.numeric_list:\n",
    "                description = df_in[col].describe()\n",
    "                col_text += f\"Cantidad valores cero: {(df_in[col] == 0).sum()}\\n\" \\\n",
    "                            + f\"Porcentaje valores cero: {((df_in[col] == 0).sum()*100/len(df_in[col])):.2f}%\\n\" \\\n",
    "                            + f\"Cantidad valores negativos: {(df_in[col] < 0).sum()}\\n\" \\\n",
    "                            + f\"Porcentaje valores negativos: {((df_in[col] < 0).sum()*100/len(df_in[col])):.2f}%\\n\" \\\n",
    "                            + f\"M√°ximo: {description['max']}\\n\" \\\n",
    "                            + f\"M√≠nimo: {description['min']}\\n\" \\\n",
    "                            + f\"Promedio: {description['mean']}\\n\" \\\n",
    "                            + f\"Percentil 25: {description['25%']}\\n\" \\\n",
    "                            + f\"Percentil 50: {description['50%']}\\n\" \\\n",
    "                            + f\"Percentil 75: {description['75%']}\\n\\n\"\n",
    "                \n",
    "                threshold = None\n",
    "                if anomaly_threshold:\n",
    "                    threshold = anomaly_threshold\n",
    "                elif anomalies and col in anomalies:\n",
    "                    threshold = anomalies[col]\n",
    "                if threshold:\n",
    "                    z_df = np.abs(self._get_Z_score(df_in[col], df_in[col].mean(), df_in[col].std()))\n",
    "                    filtered_df = df_in[z_df > threshold]\n",
    "                    if not filtered_df.empty:\n",
    "                        col_text += f\"Se encontraron {filtered_df.shape[0]} anomal√≠as en la columna {col}\\n\"\n",
    "                        if show_anomalies:\n",
    "                            col_text += f\"Anomal√≠as: {filtered_df.index.to_list()}\\n\\n\"\n",
    "                        else:\n",
    "                            col_text += '\\n'\n",
    "            print(col_text)\n",
    "            all_text.append(col_text)\n",
    "        \n",
    "        with open(self.root_path/\"summary.txt\", 'w', encoding='utf-8') as file:\n",
    "            for text in all_text:\n",
    "                file.write(text)\n",
    "    \n",
    "\n",
    "    def _cramer_V(self, col1, col2):\n",
    "        confusion_matrix = pd.crosstab(\n",
    "            index=[self.df_raw[self.df_raw.columns[col1]]],\n",
    "            columns=[self.df_raw[self.df_raw.columns[col2]]],\n",
    "        ).to_numpy()\n",
    "        chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "        n = confusion_matrix.sum()\n",
    "        phi2 = chi2/n\n",
    "        r, k = confusion_matrix.shape\n",
    "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "        rcorr = r - ((r-1)**2)/(n-1)\n",
    "        kcorr = k - ((k-1)**2)/(n-1)\n",
    "        return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "\n",
    "    def plot_vars(self, subset=[], top_categories=10):\n",
    "        folder_path = self.root_path/'plots'\n",
    "        folder_path.mkdir(exist_ok=True)\n",
    "        if not subset:\n",
    "            subset = self.df_raw.columns\n",
    "        to_correlation = []\n",
    "        for index, col in enumerate(subset):\n",
    "            if index in self.numeric_list:\n",
    "                to_correlation.append(col)\n",
    "                fig = ff.create_distplot([self.df_raw[col]], curve_type='normal', group_labels=[col], show_rug=False)\n",
    "                fig.update_layout(title_text=col)\n",
    "\n",
    "            elif index in self.datetime_list:\n",
    "                to_correlation.append(col)\n",
    "\n",
    "            elif index in self.categorical_list:\n",
    "                top = self.df_raw[col].value_counts()[:top_categories].rename(\"Count\")\n",
    "                fig = px.bar(x=top.index.astype(str), y=top, title=f\"Top {top_categories} categor√≠as m√°s comunes de {col}\")\n",
    "            else:\n",
    "                continue\n",
    "            fig.show()\n",
    "            # fig.write_image(folder_path/f\"{col}.pdf\", format=\"pdf\", engine=\"kaleido\")\n",
    "\n",
    "        fig_corr = px.imshow(self.df_raw[self.df_raw.columns[self.numeric_list]].corr())\n",
    "        fig_corr.show()\n",
    "        # fig_corr.write_image(folder_path/\"correlation.pdf\", forma='pdf', engine=\"kaleido\")\n",
    "        \n",
    "        cramer_V_matrix = np.identity(len(self.categorical_list), dtype=float)\n",
    "        for i in range(len(self.categorical_list)):\n",
    "            for j in range(i, len(self.categorical_list)):\n",
    "                cramer_V = self._cramer_V(self.categorical_list[i], self.categorical_list[j])\n",
    "                cramer_V_matrix[i, j] = cramer_V\n",
    "                cramer_V_matrix[j, i] = cramer_V\n",
    "        \n",
    "        fig_cramer = px.imshow(\n",
    "            img=cramer_V_matrix,\n",
    "            title=\"Coeficiente V de Cram√©r\",\n",
    "            x=self.df_raw.columns[self.categorical_list],\n",
    "            y=self.df_raw.columns[self.categorical_list],\n",
    "        )\n",
    "        fig_cramer.show()\n",
    "        # fig_cramer.write_image(folder_path/\"cramer_V.pdf\", forma='pdf', engine=\"kaleido\")\n",
    "\n",
    "    \n",
    "    def _clean_data(self, df, imputation):\n",
    "        for index, col in tqdm(enumerate(df.columns), desc=\"Limpiando los datos...\"):\n",
    "            if index in self.numeric_list:\n",
    "                imp = imputation['numeric']\n",
    "                if imp == 'drop':\n",
    "                    df = df.dropna(subset=col)\n",
    "                elif imp == 'fill':\n",
    "                    df[col] = df[col].fillna(df[col].mean())\n",
    "                else:\n",
    "                    raise ValueError(\"Solo se permite limpiar usando 'drop' o 'fill'\")\n",
    "            if index in self.categorical_list:\n",
    "                imp = imputation['categorical']\n",
    "                if imp == 'drop':\n",
    "                    df = df.dropna(subset=col)\n",
    "                elif imp == 'fill':\n",
    "                    df[col] = df[col].fillna(df[col].mode())\n",
    "                else:\n",
    "                    raise ValueError(\"Solo se permite limpiar usando 'drop' o 'fill'\")\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def clean_data(self, subset=[], imputation={'numeric': 'drop', 'categorical': 'drop'}):\n",
    "        folder_path = self.root_path/'clean_data'\n",
    "        folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "        df = self.df_raw if not subset else self.df_raw[subset]\n",
    "        \n",
    "        self.cleaner = skl_pre.FunctionTransformer(self._clean_data, kw_args={'imputation': imputation}) \n",
    "        self.df_processed = self.cleaner.transform(df)\n",
    "        print(\"Datos limpios!\")\n",
    "        self.df_processed.to_csv(folder_path/\"data.csv\")\n",
    "        return self.df_processed\n",
    "\n",
    "\n",
    "    def _name_combiner(input_feature, category):\n",
    "        return f\"{input_feature}_{category}\"\n",
    "\n",
    "\n",
    "    def scale(self, method=\"min-max\"):\n",
    "        folder_path = self.root_path/'scale'\n",
    "        folder_path.mkdir(exist_ok=True)\n",
    "        df = self.clean_data() if self.df_processed is None else self.df_processed\n",
    "        method_map = {\n",
    "            \"standard\": skl_pre.StandardScaler,\n",
    "            \"min-max\": skl_pre.MinMaxScaler,\n",
    "            \"max-abs\": skl_pre.MaxAbsScaler,\n",
    "            \"robust\": skl_pre.RobustScaler,\n",
    "            \"box-cox\": skl_pre.PowerTransformer,\n",
    "            \"yeo-johnson\": skl_pre.PowerTransformer,\n",
    "            \"quantile-uniform\": skl_pre.QuantileTransformer,\n",
    "            \"quantile-normal\": skl_pre.QuantileTransformer,\n",
    "            \"normalizer\": skl_pre.Normalizer,\n",
    "        }\n",
    "\n",
    "        if method in {'box-cox', 'yeo-johnson'}:\n",
    "            num_scaler = method_map[method](method=method)\n",
    "        elif 'quantile' in method:\n",
    "            num_scaler = method_map[method](output_distribution=method.split('-')[1])\n",
    "        elif method in method_map:\n",
    "            num_scaler = method_map[method]()\n",
    "        else:\n",
    "            raise ValueError(f\"Solo se permiten los siguientes m√©todos: {list(method_map.keys())}\")\n",
    "                \n",
    "        self.scaler = ColumnTransformer([\n",
    "            ('Scaler', num_scaler, self.numeric_list),\n",
    "            ('OneHot', skl_pre.OneHotEncoder(sparse_output=False), self.categorical_list)\n",
    "        ])\n",
    "        self.scaler.set_output(transform='pandas')\n",
    "        self.df_processed = self.scaler.fit_transform(df)\n",
    "        self.df_processed.to_csv(folder_path/'scaled_features.csv')\n",
    "        return self.df_processed\n",
    "\n",
    "        \n",
    "    def _elbow_method(self, df, model, n_clusters):\n",
    "        if not n_clusters:\n",
    "            return model()\n",
    "                \n",
    "        inertias = np.zeros(20, dtype=float)\n",
    "        for i in trange(20, desc=\"Optimizando n√∫mero de clusters...\"):\n",
    "            inertias[i] = model(n_clusters=i+1).fit(df).inertia_\n",
    "        \n",
    "        x = np.array(range(1, 21))\n",
    "        x = (x - np.min(x))/(np.max(x) - np.min(x))\n",
    "        inertias = (inertias - np.min(inertias))/(np.max(inertias) - np.min(inertias))\n",
    "        a = x**2 + inertias**2\n",
    "        b = (x-1)**2 + (inertias-1)**2\n",
    "        c = inertias**2\n",
    "        f = b/(a+c)\n",
    "        optimal = np.argmax(f)\n",
    "        print(f\"N√∫mero √≥ptimo de clusters: {optimal}\")\n",
    "\n",
    "        fig = px.line(x=range(20), y=inertias, title='M√©todo del codo')\n",
    "        fig.add_vline(x=optimal, line_dash='dash', line_color='red', annotation_text=f'N√∫mero de clusters √≥ptimo = {optimal}')\n",
    "        fig.show()\n",
    "        \n",
    "        return model(n_clusters=optimal)\n",
    "\n",
    "\n",
    "    def make_clusters(self, method=\"kmeans\"):\n",
    "        folder_path = self.root_path/'clusters'\n",
    "        folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "        df = self.df_processed if self.df_processed else self.scale()\n",
    "\n",
    "        method_map = {\n",
    "            \"kmeans\": skl_cl.KMeans,\n",
    "            \"affinity\": skl_cl.AffinityPropagation,\n",
    "            \"mean-shift\": skl_cl.MeanShift,\n",
    "            \"spectral\": skl_cl.SpectralClustering,\n",
    "            \"agglomerative\": skl_cl.AgglomerativeClustering,\n",
    "            \"feat-agg\": skl_cl.FeatureAgglomeration,\n",
    "            \"dbscan\": skl_cl.DBSCAN,\n",
    "            \"hdbscan\": skl_cl.HDBSCAN,\n",
    "            \"optics\": skl_cl.OPTICS,\n",
    "            \"birch\": skl_cl.Birch\n",
    "        }\n",
    "        pre_clusters_list = [\n",
    "            'kmeans',\n",
    "            'spectral',\n",
    "            'agglomerative',\n",
    "            'feat-agg',\n",
    "            'birch'\n",
    "        ]\n",
    "\n",
    "        cluster_method = method_map[method]\n",
    "        self.cluster_model = self._elbow_method(df, cluster_method, n_clusters=(method in pre_clusters_list))\n",
    "\n",
    "        clusters = self.cluster_model.fit(df)\n",
    "        df['Cluster'] = clusters.labels_\n",
    "        self.df_processed = df\n",
    "        self.df_processed.to_csv(folder_path/\"data_clusters.csv\")\n",
    "\n",
    "        self.pipeline = Pipeline([('Cleaner', self.cleaner),\n",
    "                                (\"Scaler\", self.scaler),\n",
    "                                (\"Cluster\", self.cluster_model)])\n",
    "        \n",
    "        display(self.pipeline)\n",
    "\n",
    "        reduction = PCA(n_components=2).fit_transform(self.df_processed)\n",
    "        reduction = pd.DataFrame(reduction, columns=['X', 'Y'])\n",
    "        fig = px.scatter(\n",
    "            reduction, \n",
    "            x='X', \n",
    "            y='Y', \n",
    "            color=clusters.labels_,\n",
    "            title=\"Clustering de los datos\"\n",
    "        )\n",
    "        fig.show()\n",
    "        # fig.write_image(folder_path/\"clusters.pdf\", format=\"pdf\", engine=\"kaleido\")\n",
    "        return self.df_processed\n",
    "        \n",
    "    \n",
    "\n",
    "    def detect_anomalies(self, method):\n",
    "        folder_path = self.root_path/'anomalies'\n",
    "        folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def profile(self):\n",
    "        self.summarize()\n",
    "        # self.plot_vars()\n",
    "        self.clean_data()\n",
    "        self.scale()\n",
    "        self.make_clusters()\n",
    "        self.detect_anomalies()\n",
    "        return self.df_processed\n",
    "\n",
    "\n",
    "    def clearGarbage(self):\n",
    "        for path in tqdm(os.listdir(), desc=\"Limpiando datos antiguos...\"):\n",
    "            if 'EDA' in path:\n",
    "                shutil.rmtree(path)\n",
    "        print(\"Limpieza terminada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"olimpiadas.parquet\")\n",
    "df = df.astype(\n",
    "    {\n",
    "        'Sex': 'category',\n",
    "        'Medal': 'category',\n",
    "        'Season': 'category',\n",
    "        'NOC': 'category',\n",
    "        'Sport': 'category',\n",
    "        'Event': 'category'\n",
    "    }\n",
    ")\n",
    "df.drop(['age-height-weight'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Profiler(df)\n",
    "p.make_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--plotlyjs argument is not a valid URL or file path: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Proyecto1.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nico/Desktop/uchile/MDS7202%20-%20Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/proyecto1/Proyecto1.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mline(x\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m), y\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nico/Desktop/uchile/MDS7202%20-%20Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/proyecto1/Proyecto1.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fig\u001b[39m.\u001b[39;49mwrite_image(\u001b[39m'\u001b[39;49m\u001b[39mtest.png\u001b[39;49m\u001b[39m'\u001b[39;49m, engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkaleido\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Lib\\site-packages\\plotly\\basedatatypes.py:3841\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3781\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3782\u001b[0m \u001b[39mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[0;32m   3783\u001b[0m \u001b[39mobject\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3837\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3838\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3839\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3841\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mwrite_image(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Lib\\site-packages\\plotly\\io\\_kaleido.py:266\u001b[0m, in \u001b[0;36mwrite_image\u001b[1;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 )\n\u001b[0;32m    261\u001b[0m             )\n\u001b[0;32m    263\u001b[0m     \u001b[39m# Request image\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     \u001b[39m# -------------\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[39m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     img_data \u001b[39m=\u001b[39m to_image(\n\u001b[0;32m    267\u001b[0m         fig,\n\u001b[0;32m    268\u001b[0m         \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m    269\u001b[0m         scale\u001b[39m=\u001b[39;49mscale,\n\u001b[0;32m    270\u001b[0m         width\u001b[39m=\u001b[39;49mwidth,\n\u001b[0;32m    271\u001b[0m         height\u001b[39m=\u001b[39;49mheight,\n\u001b[0;32m    272\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    273\u001b[0m         engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     \u001b[39m# Open file\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[39m# ---------\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         \u001b[39m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m         \u001b[39m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Lib\\site-packages\\plotly\\io\\_kaleido.py:143\u001b[0m, in \u001b[0;36mto_image\u001b[1;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m# Validate figure\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m# ---------------\u001b[39;00m\n\u001b[0;32m    142\u001b[0m fig_dict \u001b[39m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n\u001b[1;32m--> 143\u001b[0m img_bytes \u001b[39m=\u001b[39m scope\u001b[39m.\u001b[39;49mtransform(\n\u001b[0;32m    144\u001b[0m     fig_dict, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m, width\u001b[39m=\u001b[39;49mwidth, height\u001b[39m=\u001b[39;49mheight, scale\u001b[39m=\u001b[39;49mscale\n\u001b[0;32m    145\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m img_bytes\n",
      "File \u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Lib\\site-packages\\kaleido\\scopes\\plotly.py:103\u001b[0m, in \u001b[0;36mPlotlyScope.transform\u001b[1;34m(self, figure, format, width, height, scale)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     93\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid format \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{original_format}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m    Supported formats: \u001b[39m\u001b[39m{supported_formats_str}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[39m# Transform in using _perform_transform rather than superclass so we can access the full\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m# response dict, including error codes.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_perform_transform(\n\u001b[0;32m    104\u001b[0m     figure, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m, width\u001b[39m=\u001b[39;49mwidth, height\u001b[39m=\u001b[39;49mheight, scale\u001b[39m=\u001b[39;49mscale\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[39m# Check for export error, later can customize error messages for plotly Python users\u001b[39;00m\n\u001b[0;32m    108\u001b[0m code \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Lib\\site-packages\\kaleido\\scopes\\base.py:287\u001b[0m, in \u001b[0;36mBaseScope._perform_transform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39mTransform input data using the current scope, returning dict response with error code\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39mwhether successful or not.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39m:return: Dict of response from Kaleido executable, whether successful or not\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m# Ensure that kaleido subprocess is running\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_kaleido()\n\u001b[0;32m    289\u001b[0m \u001b[39m# Perform export\u001b[39;00m\n\u001b[0;32m    290\u001b[0m export_spec \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(\n\u001b[0;32m    291\u001b[0m     \u001b[39mdict\u001b[39m(kwargs, data\u001b[39m=\u001b[39mdata),\n\u001b[0;32m    292\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_json_encoder)\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nico\\Desktop\\uchile\\MDS7202 - Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\proyecto1\\Lib\\site-packages\\kaleido\\scopes\\base.py:200\u001b[0m, in \u001b[0;36mBaseScope._ensure_kaleido\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m startup_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proc\u001b[39m.\u001b[39mwait()\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(startup_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to start Kaleido subprocess\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: --plotlyjs argument is not a valid URL or file path: "
     ]
    }
   ],
   "source": [
    "fig = px.line(x=range(20), y=range(20))\n",
    "fig.write_image('test.png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
